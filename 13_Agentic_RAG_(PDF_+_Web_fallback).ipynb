{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyfu8a/J7y6WRDsC2B7WV4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anirudho747/Edrk_Google-Collab/blob/main/13_Agentic_RAG_(PDF_%2B_Web_fallback).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Single Colab cell: Final Agentic RAG (PDF + Web fallback) â€” copy-paste & run\n",
        "# -------------------------------------------------------------------------\n",
        "# (If you haven't installed dependencies yet, this will install them. If already installed, it's fine.)\n",
        "!pip install -q chromadb langchain pypdf gradio langchain-community\n",
        "!pip install -q google-generativeai langchain-google-genai\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q google-search-results serpapi\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Imports & robust SerpApi client selection\n",
        "# -------------------------------------------------------------------------\n",
        "import os, json, uuid, traceback, time\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# prefer google-search-results client first, fallback to serpapi variants\n",
        "try:\n",
        "    from google_search_results import GoogleSearch as SerpGoogleSearch\n",
        "    print(\"Using google_search_results.GoogleSearch\")\n",
        "except Exception:\n",
        "    try:\n",
        "        from serpapi import GoogleSearch as SerpGoogleSearch\n",
        "        print(\"Using serpapi.GoogleSearch\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            from serpapi.google_search_results import GoogleSearch as SerpGoogleSearch\n",
        "            print(\"Using serpapi.google_search_results.GoogleSearch\")\n",
        "        except Exception as e:\n",
        "            raise ImportError(\"Could not import GoogleSearch client. Install google-search-results or serpapi. Error: \" + str(e))\n",
        "\n",
        "# LangChain / LLM / helper imports\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document, HumanMessage\n",
        "import gradio as gr\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# API keys: optionally use Colab userdata or set manually before running\n",
        "# -------------------------------------------------------------------------\n",
        "if userdata.get(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if userdata.get(\"SERPAPI_API_KEY\"):\n",
        "    os.environ[\"SERPAPI_API_KEY\"] = userdata.get(\"SERPAPI_API_KEY\")\n",
        "\n",
        "if not os.environ.get(\"SERPAPI_API_KEY\"):\n",
        "    print(\"Warning: SERPAPI_API_KEY not set. Web fallback will be disabled until you set it.\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1) Upload & load PDF (single upload)\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"Upload your PDF now (Colab will prompt you).\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise FileNotFoundError(\"No file uploaded. Re-run cell and upload the PDF.\")\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "print(\"Uploaded PDF:\", pdf_filename)\n",
        "\n",
        "loader = PyPDFLoader(pdf_filename)\n",
        "pages = loader.load()\n",
        "print(f\"Loaded {len(pages)} pages from PDF.\")\n",
        "\n",
        "# Split into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "docs = splitter.split_documents(pages)\n",
        "print(f\"Split into {len(docs)} chunks.\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2) Persistent embeddings + vectorstore (create once)\n",
        "# -------------------------------------------------------------------------\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(docs, embeddings, collection_name=f\"pdf_collection_{uuid.uuid4().hex[:8]}\")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "print(\"Persistent vectorstore created.\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3) LLM + QA chain\n",
        "# -------------------------------------------------------------------------\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4) Robust extract & LLM-call helpers (avoid .content on str, use HumanMessage)\n",
        "# -------------------------------------------------------------------------\n",
        "def extract_text_from_llm_response(raw):\n",
        "    \"\"\"Return a plain string for many common return shapes.\"\"\"\n",
        "    try:\n",
        "        if raw is None:\n",
        "            return \"\"\n",
        "        if isinstance(raw, str):\n",
        "            return raw\n",
        "        if isinstance(raw, dict):\n",
        "            for k in (\"output_text\", \"text\", \"answer\", \"content\"):\n",
        "                if k in raw and isinstance(raw[k], str):\n",
        "                    return raw[k]\n",
        "            # pick first string value if any\n",
        "            for v in raw.values():\n",
        "                if isinstance(v, str):\n",
        "                    return v\n",
        "            return json.dumps(raw)\n",
        "        # object with .content\n",
        "        if hasattr(raw, \"content\"):\n",
        "            c = getattr(raw, \"content\")\n",
        "            if isinstance(c, str):\n",
        "                return c\n",
        "            if isinstance(c, (list, tuple)) and len(c) > 0:\n",
        "                return c[0] if isinstance(c[0], str) else str(c[0])\n",
        "            return str(c)\n",
        "        # langchain-like .generations\n",
        "        if hasattr(raw, \"generations\"):\n",
        "            try:\n",
        "                return raw.generations[0][0].text\n",
        "            except Exception:\n",
        "                return str(raw)\n",
        "        # .text fallback\n",
        "        if hasattr(raw, \"text\"):\n",
        "            t = getattr(raw, \"text\")\n",
        "            return t if isinstance(t, str) else str(t)\n",
        "        return str(raw)\n",
        "    except Exception as e:\n",
        "        return f\"<unextractable: {type(raw).__name__}: {e}>\"\n",
        "\n",
        "def call_llm_with_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Call global llm robustly. Use HumanMessage for generate to satisfy langchain_core expectations.\n",
        "    Always return a plain string (never an object).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Preferred: generate expects list-of-lists of BaseMessage\n",
        "        if hasattr(llm, \"generate\"):\n",
        "            raw = llm.generate([[HumanMessage(content=prompt)]])\n",
        "            return extract_text_from_llm_response(raw)\n",
        "        # next: invoke if available\n",
        "        if hasattr(llm, \"invoke\"):\n",
        "            raw = llm.invoke(prompt)\n",
        "            return extract_text_from_llm_response(raw)\n",
        "        # fallback: try callable\n",
        "        raw = llm(prompt)\n",
        "        return extract_text_from_llm_response(raw)\n",
        "    except Exception as e:\n",
        "        tb = traceback.format_exc()\n",
        "        return f\"LLM call failed: {type(e).__name__}: {e}\\n{tb}\"\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 5) In-doc classifier (LLM decides if question is answerable from PDF snippets)\n",
        "# -------------------------------------------------------------------------\n",
        "def llm_is_in_doc(query, top_snippets):\n",
        "    snippet_text = \"\\n\\n---\\n\\n\".join(f\"SNIPPET {i+1}:\\n{txt[:1200]}\" for i, txt in enumerate(top_snippets)) or \"No snippets found.\"\n",
        "    safe_query = json.dumps(query)\n",
        "    prompt = f\"\"\"\n",
        "You are an assistant whose job is to decide whether a user's question can be answered using only the provided PDF snippets.\n",
        "User question: {safe_query}\n",
        "\n",
        "Below are the top PDF snippets retrieved by a semantic retriever. Answer ONLY in strict JSON format with two fields:\n",
        "- in_doc: true or false\n",
        "- explanation: one-sentence explanation why.\n",
        "\n",
        "Do NOT include any other text.\n",
        "\n",
        "PDF SNIPPETS:\n",
        "{snippet_text}\n",
        "\"\"\"\n",
        "    text = call_llm_with_prompt(prompt)\n",
        "    # try parse JSON\n",
        "    try:\n",
        "        start = text.find(\"{\")\n",
        "        end = text.rfind(\"}\")\n",
        "        if start != -1 and end != -1 and end > start:\n",
        "            parsed = json.loads(text[start:end+1])\n",
        "            in_doc = bool(parsed.get(\"in_doc\") is True or str(parsed.get(\"in_doc\")).lower() in [\"true\", \"yes\"])\n",
        "            explanation = parsed.get(\"explanation\", \"\")\n",
        "            return {\"in_doc\": in_doc, \"explanation\": explanation, \"raw\": text}\n",
        "    except Exception:\n",
        "        pass\n",
        "    # fallback heuristics\n",
        "    low = text.lower()\n",
        "    if any(w in low for w in (\"in_doc\", \"true\", \"yes\", \"answerable\", \"can be answered\")):\n",
        "        return {\"in_doc\": True, \"explanation\": text, \"raw\": text}\n",
        "    return {\"in_doc\": False, \"explanation\": text, \"raw\": text}\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6) SerpApi search helper (defensive)\n",
        "# -------------------------------------------------------------------------\n",
        "def serpapi_search(query, num=3):\n",
        "    if not os.environ.get(\"SERPAPI_API_KEY\"):\n",
        "        # web disabled â€” return empty so chatbot can reply gracefully\n",
        "        print(\"serpapi_search: SERPAPI_API_KEY not set; skipping web search.\")\n",
        "        return []\n",
        "    params = {\"engine\": \"google\", \"q\": query, \"num\": num, \"api_key\": os.environ[\"SERPAPI_API_KEY\"]}\n",
        "    try:\n",
        "        search = SerpGoogleSearch(params)\n",
        "        result = search.get_dict()\n",
        "    except Exception as e:\n",
        "        print(f\"serpapi_search: query failed: {type(e).__name__}: {e}\")\n",
        "        return []\n",
        "    hits = []\n",
        "    try:\n",
        "        for r in result.get(\"organic_results\", [])[:num]:\n",
        "            title = r.get(\"title\") or \"\"\n",
        "            snippet = r.get(\"snippet\") or r.get(\"snippet_html\") or \"\"\n",
        "            link = r.get(\"link\") or r.get(\"displayed_link\") or \"\"\n",
        "            hits.append((title, snippet, link))\n",
        "    except Exception:\n",
        "        pass\n",
        "    if not hits:\n",
        "        try:\n",
        "            for r in result.get(\"top_results\", [])[:num]:\n",
        "                hits.append((r.get(\"title\", \"\"), r.get(\"snippet\", \"\"), r.get(\"link\", \"\")))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return hits\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 7) Prompt-based answer synthesis (no per-query Chroma rebuild)\n",
        "# -------------------------------------------------------------------------\n",
        "def answer_from_context(query, pdf_snips, web_hits):\n",
        "    pdf_context = \"\\n\\n\".join(f\"PDF_SNIPPET {i+1}:\\n{txt[:1200]}\" for i, txt in enumerate(pdf_snips))\n",
        "    web_context = \"\\n\\n\".join(f\"WEB_{i+1}: Title: {t}\\nSnippet: {s}\\nURL: {u}\" for i, (t, s, u) in enumerate(web_hits))\n",
        "    context = \"\\n\\n\".join([c for c in [pdf_context, web_context] if c.strip()])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an assistant that must answer the user's question using ONLY the information in the provided sources (PDF snippets and web snippets).\n",
        "Cite sources inline using [PDF_SNIPPET #] or [WEB_#] when you reference them. If the answer cannot be found in the sources, say 'I don't know based on the given sources.'\n",
        "\n",
        "User question:\n",
        "\\\"\\\"\\\"{query}\\\"\\\"\\\"\n",
        "\n",
        "SOURCES:\n",
        "{context}\n",
        "\n",
        "Now produce a clear, concise answer and include the source references used.\n",
        "\"\"\"\n",
        "    return call_llm_with_prompt(prompt)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 8) Utility: get top pdf snippets from persistent retriever\n",
        "# -------------------------------------------------------------------------\n",
        "def get_top_pdf_snippets(query, k=3):\n",
        "    docs_local = retriever.get_relevant_documents(query)\n",
        "    return [d.page_content for d in (docs_local[:k] if docs_local else [])]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 9) Chatbot handler (robust)\n",
        "# -------------------------------------------------------------------------\n",
        "def chatbot(query):\n",
        "    try:\n",
        "        if not isinstance(query, str) or query.strip() == \"\":\n",
        "            return \"Please type a question and press Enter.\"\n",
        "\n",
        "        query = query.strip()\n",
        "\n",
        "        # 1) Get top pdf snippets\n",
        "        top_snips = get_top_pdf_snippets(query, k=3)\n",
        "\n",
        "        # 2) LLM decides whether in-doc\n",
        "        check = llm_is_in_doc(query, top_snips)\n",
        "        if check.get(\"in_doc\"):\n",
        "            try:\n",
        "                return qa_chain.run(query)\n",
        "            except Exception:\n",
        "                # fallback to prompt-based synth from pdf snippets\n",
        "                return answer_from_context(query, top_snips, [])\n",
        "\n",
        "        # 3) Out-of-doc: web fallback\n",
        "        web_hits = serpapi_search(query, num=3)\n",
        "        if not web_hits:\n",
        "            return f\"I don't know based on the given sources. (LLM decision: {check.get('explanation')})\"\n",
        "\n",
        "        # 4) Synthesize answer from pdf + web snippets\n",
        "        return answer_from_context(query, top_snips, web_hits)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in chatbot pipeline: {type(e).__name__}: {e}\"\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 10) Gradio UI: queued and disable flagging\n",
        "# -------------------------------------------------------------------------\n",
        "demo = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.Textbox(label=\"Ask HR Assistant a question\", lines=3, placeholder=\"Type your HR question here...\"),\n",
        "    outputs=gr.Textbox(label=\"Answer\", lines=12),\n",
        "    title=\"AI-Powered HR Assistant (PDF + Web fallback)\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "print(\"Launching Gradio app. Upload complete; the UI will appear below with a public link while the Colab session is active.\")\n",
        "demo.launch(share=True)\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n7vE3XIEIB4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q chromadb langchain pypdf gradio langchain-community\n",
        "!pip install -q google-generativeai langchain-google-genai\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q serpapi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EEN2ZT9AGDj",
        "outputId": "d4ef7886-051d-4a82-a85f-85ed82dddcff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-919991598.py:241: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs_local = retriever.get_relevant_documents(query)\n",
            "/tmp/ipython-input-919991598.py:261: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return qa_chain.run(query)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback, sys\n",
        "modules = [\"serpapi\", \"langchain_community\", \"langchain_google_genai\", \"langchain\", \"chromadb\", \"sentence_transformers\"]\n",
        "for m in modules:\n",
        "    try:\n",
        "        __import__(m)\n",
        "        print(\"OK:\", m)\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\", m, type(e).__name__, e)\n",
        "        traceback.print_exc()\n",
        "print(\"Python:\", sys.executable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnb31W-NGbW5",
        "outputId": "67c6bdf3-3799-4cd4-c6da-046a060cc5ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: serpapi\n",
            "OK: langchain_community\n",
            "OK: langchain_google_genai\n",
            "OK: langchain\n",
            "OK: chromadb\n",
            "OK: sentence_transformers\n",
            "Python: /usr/bin/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-search-results\n",
        "!pip install -q 'serpapi==2.1.0' || true\n"
      ],
      "metadata": {
        "id": "-gzIAPRbU1rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "print(\"Restarting runtime to pick up newly installed packages...\")\n",
        "sys.stdout.flush()\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQZfB8x-VFsi",
        "outputId": "cfa4aafb-0b31-4432-912c-430e844b396e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restarting runtime to pick up newly installed packages...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worked = False\n",
        "try:\n",
        "    from google_search_results import GoogleSearch as GSR_GoogleSearch\n",
        "    print(\"OK: google_search_results.GoogleSearch (preferred).\")\n",
        "    worked = True\n",
        "except Exception as e:\n",
        "    print(\"google_search_results import failed:\", type(e).__name__, e)\n",
        "\n",
        "if not worked:\n",
        "    try:\n",
        "        from serpapi import GoogleSearch as SerpGoogleSearch\n",
        "        print(\"OK: serpapi.GoogleSearch\")\n",
        "        worked = True\n",
        "    except Exception as e:\n",
        "        print(\"serpapi.GoogleSearch import failed:\", type(e).__name__, e)\n",
        "\n",
        "if not worked:\n",
        "    try:\n",
        "        from serpapi.google_search_results import GoogleSearch as SerpGoogleSearchAlt\n",
        "        print(\"OK: serpapi.google_search_results.GoogleSearch (alternate).\")\n",
        "        worked = True\n",
        "    except Exception as e:\n",
        "        print(\"serpapi.google_search_results import failed:\", type(e).__name__, e)\n",
        "\n",
        "print(\"Final status:\", \"OK\" if worked else \"FAILED\")\n"
      ],
      "metadata": {
        "id": "4APjXk6jVL4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}