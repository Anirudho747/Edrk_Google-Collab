{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGGfMfT80hxZD/g22ybVn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anirudho747/Edrk_Google-Collab/blob/main/Agentic_Rag_%2B_Crew_AI_%2B_SerpAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFKxQqoakxFr",
        "outputId": "e3260f48-c94b-46ff-fb73-55014d21156f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding length: 384\n",
            "\u001b[96mUsing Tool: SerpAPI Search\u001b[0m\n",
            "- What advice would you give to your colleagues who are ...\n",
            "  https://www.reddit.com/r/webdev/comments/1ip6txq/what_advice_would_you_give_to_your_colleagues_who/\n",
            "  I noticed that a lot of Test Automation Specialists are getting laid off. I know a lot of these folks have a bad reputation for being ...\n",
            "- Why We Fired All Testers and What Happened Next\n",
            "  https://www.linkedin.com/posts/shaikshoaibrehman_testers-dev-qa-activity-7376829052793442305-bs5x\n",
            "  The Day Came, We Fired All Testers Management  ...\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# CELL 1: Install libraries for CrewAI + Gemini + SerpAPI +\n",
        "#         Hugging Face Embeddings + FAISS + PDF loader\n",
        "# ---------------------------------------------------------\n",
        "# Notes:\n",
        "# - We pin requests==2.32.5 for compatibility with crewai-tools (warnings about Colab's 2.32.4 are fine to ignore).\n",
        "# - We use langchain-huggingface to avoid deprecation warnings for HF embeddings in langchain_community.\n",
        "# - We install google-search-results (SerpAPI official client).\n",
        "# - We add readability + bs4 in case you later want a custom scraper fallback.\n",
        "\n",
        "%pip install -q \\\n",
        "  requests==2.32.5 \\\n",
        "  \"crewai>=0.65.0\" \"crewai-tools>=1.1.0\" \\\n",
        "  \"langchain==0.3.27\" \"langchain-core==0.3.79\" \"langchain-community==0.3.27\" \\\n",
        "  langchain-huggingface \\\n",
        "  google-search-results \\\n",
        "  faiss-cpu chromadb pypdf python-dotenv sentence-transformers \\\n",
        "  beautifulsoup4 readability-lxml lxml-html-clean\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CELL 2: Imports + Key loading + LLM + Embeddings + Search\n",
        "# ---------------------------------------------------------\n",
        "import os, re, textwrap\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# CrewAI core\n",
        "from crewai import Agent, Task, Crew, LLM\n",
        "\n",
        "# Tool base class (for custom tools) and built-in scraper\n",
        "from crewai.tools import BaseTool                  # Base class for your own tools\n",
        "from crewai_tools import ScrapeWebsiteTool        # Built-in website scraper (your choice C1)\n",
        "\n",
        "# LangChain utilities for RAG\n",
        "from langchain_huggingface import HuggingFaceEmbeddings  # Current, non-deprecated HF embeddings in LC\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# SerpAPI (official client library)\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "# --- Keys ---\n",
        "# Preferred: upload a .env file (with GEMINI and SERPAPI_API_KEY) and load:\n",
        "load_dotenv()\n",
        "\n",
        "# SECURITY: Do NOT hard-code keys as defaults. If these asserts fire, set keys in Cell 0 or via .env.\n",
        "GEMINI = os.environ.get(\"GEMINI\", \"AIzaSyCKCwFXiwwXRGXON-j6w8GGf1EsXumg8nI\")\n",
        "SERPAPI_API_KEY = os.environ.get(\"SERPAPI_API_KEY\", \"4b05812c3a17221a9d9deac6dfe2ff9658cceb2068d4bc53095c4a76b4c3313b\")\n",
        "assert GEMINI, \"Set GEMINI key in env or Cell 0\"\n",
        "assert SERPAPI_API_KEY, \"Set SERPAPI_API_KEY in env or Cell 0\"\n",
        "\n",
        "# --- LLM ---\n",
        "# A1: Gemini 1.5 Flash (fast & cheap). Swap to \"gemini/gemini-1.5-pro\" for more reasoning depth.\n",
        "llm = LLM(\n",
        "    model=\"gemini/gemini-1.5-flash\",\n",
        "    api_key=GEMINI,\n",
        "    temperature=0.2,   # low temp = more deterministic, better for tools & summaries\n",
        "    max_tokens=800     # increase to 1200–1600 for longer summaries\n",
        ")\n",
        "\n",
        "# --- Embeddings ---\n",
        "# D1: SBERT (MiniLM) is a great default for semantic search (speed + quality).\n",
        "# Alternatives:\n",
        "#  - Google \"text-embedding-004\" (API cost, good quality)\n",
        "#  - OpenAI \"text-embedding-3-small/large\" (API cost, good quality)\n",
        "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- SerpAPI helper ---\n",
        "# Simple function to call SerpAPI and return organic result objects\n",
        "def serp_search_results(query: str, num: int = 5):\n",
        "    num = max(1, min(int(num), 10))  # SerpAPI returns up to 10 per call\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"num\": num,\n",
        "        \"api_key\": SERPAPI_API_KEY,\n",
        "        \"hl\": \"en\"\n",
        "    }\n",
        "    data = GoogleSearch(params).get_dict() or {}\n",
        "    return data.get(\"organic_results\", []) or []\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CELL 3: SerpAPI Tool (CrewAI BaseTool)\n",
        "# ---------------------------------------------------------\n",
        "# Why: CrewAI's Task(tools=[...]) expects subclasses of BaseTool with:\n",
        "#  - a Pydantic args_schema describing inputs\n",
        "#  - an implementation of _run(...)\n",
        "# This gives validation, logging, and consistent tool calling.\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class SerpInput(BaseModel):\n",
        "    query: str = Field(..., description=\"Google search query\")\n",
        "    num: int = Field(5, ge=1, le=10, description=\"Number of results to return (1-10)\")\n",
        "\n",
        "class SerpAPISearchTool(BaseTool):\n",
        "    name: str = \"SerpAPI Search\"\n",
        "    description: str = (\n",
        "        \"Search Google via SerpAPI and return 3–5 bullets with title, link, and snippet, \"\n",
        "        \"plus a final line BEST_URL: <url>.\"\n",
        "    )\n",
        "    args_schema: type[BaseModel] = SerpInput\n",
        "\n",
        "    def _run(self, query: str, num: int = 5) -> str:\n",
        "        results = serp_search_results(query, num=num)\n",
        "        if not results:\n",
        "            return \"No results.\"\n",
        "        lines = []\n",
        "        for item in results[:num]:\n",
        "            title = item.get(\"title\", \"\")\n",
        "            link = item.get(\"link\", \"\")\n",
        "            snippet = item.get(\"snippet\", \"\")\n",
        "            lines.append(f\"- {title}\\n  {link}\\n  {snippet}\")\n",
        "        # Simple heuristic: first result as BEST_URL (you can add scoring later)\n",
        "        lines.append(f\"BEST_URL: {results[0].get('link','')}\")\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "# Instantiate tools (your choice C1)\n",
        "search_tool = SerpAPISearchTool()\n",
        "scrape_tool = ScrapeWebsiteTool()  # If a site blocks this, our prompt tells the agent to emit SCRAPE_SKIPPED.\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CELL 4: Agents, Tasks, and Crew + PDF→FAISS RAG helpers\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Agents: keep roles/goals simple and specific\n",
        "web_search_agent = Agent(\n",
        "    role=\"Web Researcher\",\n",
        "    goal=\"Find high-signal sources for the user's topic with links\",\n",
        "    backstory=\"Fast, precise, source-focused researcher.\",\n",
        "    llm=llm, verbose=True, allow_delegation=False\n",
        ")\n",
        "\n",
        "web_scraper_agent = Agent(\n",
        "    role=\"Web Scraper\",\n",
        "    goal=\"Scrape and summarize the selected page plainly and accurately\",\n",
        "    backstory=\"Detail-oriented content extractor.\",\n",
        "    llm=llm, verbose=True, allow_delegation=False\n",
        ")\n",
        "\n",
        "# Task 1: Search → bullets + BEST_URL\n",
        "# Why: We constrain the format so Task 2 can deterministically pick a URL.\n",
        "search_task = Task(\n",
        "    description=(\n",
        "        \"Search the web for '{topic}'. Return exactly 3–5 bullet points with title, link, and snippet.\\n\"\n",
        "        \"Then on a final line, output: BEST_URL: <the single best URL>.\"\n",
        "    ),\n",
        "    expected_output=\"3–5 bullets + final line 'BEST_URL: <url>'\",\n",
        "    tools=[search_tool],\n",
        "    agent=web_search_agent,\n",
        ")\n",
        "\n",
        "# Task 2: Scrape BEST_URL using the selected tool.\n",
        "# If blocked, the agent must output SCRAPE_SKIPPED (our downstream code handles it).\n",
        "scrape_task = Task(\n",
        "    description=(\n",
        "        \"From the previous output, extract the 'BEST_URL'. \"\n",
        "        \"Use ScrapeWebsiteTool on that URL and summarize the main points in plain language for a beginner.\\n\"\n",
        "        \"If the site blocks scraping or returns empty text, DO NOT fail — instead output exactly: 'SCRAPE_SKIPPED'.\"\n",
        "    ),\n",
        "    expected_output=\"A concise summary of the page, or 'SCRAPE_SKIPPED' if blocked.\",\n",
        "    tools=[scrape_tool],\n",
        "    agent=web_scraper_agent,\n",
        ")\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[web_search_agent, web_scraper_agent],\n",
        "    tasks=[search_task, scrape_task],\n",
        "    verbose=1,\n",
        "    memory=False\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# PDF → FAISS RAG helpers\n",
        "# -------------------------\n",
        "# Why FAISS: fast in-memory vector index; great for Colab.\n",
        "# Alternatives: Chroma (local/simple), Qdrant/Milvus (server), LanceDB (local with file backing).\n",
        "\n",
        "def build_vector_db(pdf_path: str):\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "    docs = PyPDFLoader(pdf_path).load()  # loads pages as Documents\n",
        "    # Split into overlapping chunks to improve retrieval coverage and context quality\n",
        "    chunks = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,   # tune for your content (500–1500 typical)\n",
        "        chunk_overlap=80   # small overlap preserves context across splits\n",
        "    ).split_documents(docs)\n",
        "    return FAISS.from_documents(chunks, emb)\n",
        "\n",
        "def retrieve_context(vdb, query: str, k: int = 5) -> str:\n",
        "    # k: how many similar chunks to pull (trade-off between recall and noise)\n",
        "    docs = vdb.similarity_search(query, k=k)\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "def can_answer_locally(query: str, context: str) -> bool:\n",
        "    # Lightweight router: ask LLM if the retrieved context is enough\n",
        "    # Alternatives:\n",
        "    #  - Heuristic: require min token length or keyword match\n",
        "    #  - Confidence scoring via retriever metadata (distances) and threshold\n",
        "    prompt = (\n",
        "        \"Answer strictly 'Yes' or 'No'.\\n\"\n",
        "        \"Does the following text contain enough information to answer the question?\\n\\n\"\n",
        "        f\"Question: {query}\\n\\nText:\\n{context[:4000]}\\n\"\n",
        "    )\n",
        "    return llm(prompt).strip().lower().startswith(\"y\")\n",
        "\n",
        "def answer_from_context(query: str, context: str) -> str:\n",
        "    # Force grounded answers; admit insufficiency when needed\n",
        "    prompt = (\n",
        "        \"Use ONLY the provided context to answer accurately. If insufficient, say so plainly.\\n\\n\"\n",
        "        f\"Context:\\n{context[:16000]}\\n\\nQuestion: {query}\\n\"\n",
        "    )\n",
        "    return llm(prompt)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CELL 5: Route: local PDF RAG first, web fallback second\n",
        "# ---------------------------------------------------------\n",
        "# Strategy:\n",
        "# 1) Retrieve from your PDF (cheap, fast, private). If enough, answer and stop.\n",
        "# 2) Otherwise, run Crew pipeline (search → scrape) and summarize.\n",
        "\n",
        "def answer_query(query: str, vdb=None, web_fallback: bool = True):\n",
        "    # 1) Try local RAG\n",
        "    local_ctx = retrieve_context(vdb, query, k=5) if vdb else \"\"\n",
        "    if local_ctx and can_answer_locally(query, local_ctx):\n",
        "        return {\"source\": \"local\", \"context\": local_ctx, \"answer\": answer_from_context(query, local_ctx)}\n",
        "\n",
        "    if not web_fallback:\n",
        "        return {\"source\": \"none\", \"context\": local_ctx, \"answer\": \"Insufficient local context.\"}\n",
        "\n",
        "    # 2) Web route (Crew): search + scrape\n",
        "    try:\n",
        "        result = crew.kickoff(inputs={\"topic\": query})\n",
        "        web_ctx = str(result.raw)  # combine outputs from both tasks\n",
        "    except Exception:\n",
        "        # If scraping tool throws, degrade gracefully: keep only search bullets\n",
        "        web_ctx = search_tool.run(query=query, num=5)\n",
        "\n",
        "    # If scraper was blocked, keep bullets + BEST_URL only (skip empty/blocked scrape content)\n",
        "    if \"SCRAPE_SKIPPED\" in web_ctx:\n",
        "        web_ctx = \"\\n\".join(\n",
        "            line for line in web_ctx.splitlines()\n",
        "            if line.startswith(\"- \") or line.startswith(\"BEST_URL:\")\n",
        "        )\n",
        "\n",
        "    ans = answer_from_context(query, web_ctx)\n",
        "    return {\"source\": \"web\", \"context\": web_ctx[:16000], \"answer\": ans}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CELL 6: Quick validation\n",
        "# ---------------------------------------------------------\n",
        "print(\"Embedding length:\", len(emb.embed_query(\"hello world\")))\n",
        "\n",
        "# Test SerpAPI tool directly (named args for tools)\n",
        "print(search_tool.run(query=\"Automation Testers Fired\", num=7)[:500], \"...\")\n",
        "\n",
        "# Optional: Build FAISS from your PDF, then ask\n",
        "# vdb = build_vector_db(\"/content/your.pdf\")\n",
        "# out = answer_query(\"What is Agentic RAG?\", vdb=vdb, web_fallback=True)\n",
        "# print(\"Source:\", out[\"source\"])\n",
        "# print(out[\"answer\"][:1200])"
      ]
    }
  ]
}